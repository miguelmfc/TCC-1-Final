**Referência:**  
Trübenbach, D.; Müller, S.; Grunske, L. "A Comparative Evaluation on the Quality of Manual and Automatic Test Case Generation Techniques for Scientific Software." SBST'22, May 2022. doi: 10.1145/3526072.3527523

### 1. Fichamento de Conteúdo
Este artigo compara a eficácia entre testes criados de forma manual e testes gerados automaticamente para o projeto científico ASE em Python. A ferramenta Pynguin foi usada com algoritmos como DynaMOSA, MIO e Whole Suite. A avaliação utilizou métricas de cobertura de branches e mutações. Como resultado se analisa que os testes manuais ultrapassam significativamente os automáticos, como pela dificuldade em lidar com entradas complexas e a estrutura dos dados científicos. O estudo conclui que é necessária a evolução das ferramentas de geração automática para alcançar níveis aceitáveis de cobertura e eficácia em sistemas científicos.

### 2. Fichamento Bibliográfico
- Projeto analisado: ASE (Atomic Simulation Environment) (p. 2).
- Ferramentas e algoritmos: Pynguin com MIO, DynaMOSA, Whole Test Suite (p. 3).
- Métricas: branch coverage, mutation coverage (p. 3-4).
- Nenhum algoritmo automático superou os testes manuais (p. 5).
- Dificuldade com entradas complexas e estruturadas (p. 6).

### 3. Fichamento de Citações
- "None of the automated test case generation algorithms manage to come close to the coverages reached by the manually created test suite."
- "Testing techniques need to be integrated into the development workflows."
- "System states that are difficult to reach... appear to be a general problem."

---
